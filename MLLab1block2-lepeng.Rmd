---
title: "Machine Learning Lab1-block2"
author: "Lepeng Zhang, Xuan Wang, Priyarani Patil"
date: "2023-12-03"
output: 
  pdf_document:
    latex_engine: xelatex
    keep_tex: yes
header-includes: 
    - \usepackage{placeins}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Statement of Contribution
The group report was made based on the discussion after all of us had finished all three assignments. Assignment 1 was mainly contributed by Lepeng Zhang. Assignment 2 was mainly contributed by Xuan Wang and Priyarani Patil.

# Assignment 1. ENSEMBLE METHODS        
### Q1      
```{r, Assignment1_1, echo = FALSE}
library(randomForest)
library(knitr)

set.seed(1234)
x1<-runif(1000)
x2<-runif(1000)
tedata<-cbind(x1,x2)
y<-as.numeric(x1<x2)
telabels<-as.factor(y)
#plot(x1,x2,col=(y+1))

num_tree <- c(1,10,100)
num_repeat <- 1000
store_mat <- matrix(nrow = num_repeat, ncol = length(num_tree))
for (i in 1:length(num_tree)){
  for (j in 1:num_repeat){
    x1<-runif(100)
    x2<-runif(100)
    trdata<-cbind(x1,x2)
    y<-as.numeric(x1<x2)
    trlabels<-as.factor(y)
    
    m1 <- randomForest(x = trdata, y = trlabels, ntree = num_tree[i], nodesize = 25, keep.forest = TRUE)
    predicted_labels <- predict(m1, newdata = tedata)
    misclassification_error <- mean(predicted_labels != telabels)
    
    store_mat[j,i] <- misclassification_error
  }
}

mean_values <- colMeans(store_mat)
variance_values <- apply(store_mat, 2, var)

result_df <- data.frame(Mean = mean_values, Variance = variance_values)
rownames(result_df) <- c("1 tree", "10 trees", "100 trees")
kable(result_df, caption = "Classification Error", format = "markdown")
```

### Q2    
```{r, Assignment1_2, echo = FALSE}
set.seed(1234)
x1<-runif(1000)
x2<-runif(1000)
tedata<-cbind(x1,x2)
y<-as.numeric(x1<0.5)
telabels<-as.factor(y)
#plot(x1,x2,col=(y+1))

num_tree <- c(1,10,100)
num_repeat <- 1000
store_mat <- matrix(nrow = num_repeat, ncol = length(num_tree))
for (i in 1:length(num_tree)){
  for (j in 1:num_repeat){
    x1<-runif(100)
    x2<-runif(100)
    trdata<-cbind(x1,x2)
    y<-as.numeric(x1<0.5)
    trlabels<-as.factor(y)
    
    m1 <- randomForest(x = trdata, y = trlabels, ntree = num_tree[i], nodesize = 25, keep.forest = TRUE)
    predicted_labels <- predict(m1, newdata = tedata)
    misclassification_error <- mean(predicted_labels != telabels)
    
    store_mat[j,i] <- misclassification_error
  }
}

mean_values <- colMeans(store_mat)
variance_values <- apply(store_mat, 2, var)

result_df <- data.frame(Mean = mean_values, Variance = variance_values)
rownames(result_df) <- c("1 tree", "10 trees", "100 trees")
kable(result_df, caption = "Classification Error", format = "markdown")
```

### Q3    
```{r, Assignment1_3, echo = FALSE}
set.seed(1234)
x1<-runif(1000)
x2<-runif(1000)
tedata<-cbind(x1,x2)
y<-as.numeric((x1<0.5&x2<0.5)|(x1>0.5&x2>0.5))
telabels<-as.factor(y)
#plot(x1,x2,col=(y+1))

num_tree <- c(1,10,100)
num_repeat <- 1000
store_mat <- matrix(nrow = num_repeat, ncol = length(num_tree))
for (i in 1:length(num_tree)){
  for (j in 1:num_repeat){
    x1<-runif(100)
    x2<-runif(100)
    trdata<-cbind(x1,x2)
    y<-as.numeric((x1<0.5&x2<0.5)|(x1>0.5&x2>0.5))
    trlabels<-as.factor(y)
    
    m1 <- randomForest(x = trdata, y = trlabels, ntree = num_tree[i], nodesize = 12, keep.forest = TRUE)
    predicted_labels <- predict(m1, newdata = tedata)
    misclassification_error <- mean(predicted_labels != telabels)
    
    store_mat[j,i] <- misclassification_error
  }
}

mean_values <- colMeans(store_mat)
variance_values <- apply(store_mat, 2, var)

result_df <- data.frame(Mean = mean_values, Variance = variance_values)
rownames(result_df) <- c("1 tree", "10 trees", "100 trees")
kable(result_df, caption = "Classification Error", format = "markdown")
```


### Q4  





# Assignment 2. MIXTURE MODELS              
## Implementation explanation    
###  Computation of the weights        
According to the literature and instruction,    
$$w_i(m)=p(y_i=m|x_i,\hat{\theta})=\frac{\hat{\pi_m}Bern(x_i|\mu_m)}{\sum_{j=1}^{M}\hat{\pi_j}Bern(x_i|\mu_j)}$$
We calculate $\hat{\pi_m}Bern(x_i|\mu_m)$ using $log$ operation first and then $exp$ operation. The reason is that it's much more convenient to use $log$ for a product term and $exp$ operation is for setting it back.   
$$\hat{\pi_m}Bern(x_i|\mu_m)=exp[log(\hat{\pi_m}Bern(x_i|\mu_m))]=exp[log(\hat{\pi_m})+\sum_{d=1}^{D}(x_{i,d}log(\mu_{m,d})+(1-x_{i,d})log(1-\mu_{m,d}))]$$

$w_i(m)$ can easily be computed after getting all $\hat{\pi_m}Bern(x_i|\mu_m)$ with $m$ from 1 to $M$.    

### Log likelihood computation       
$$llik[it]=\sum_{i=1}^{n}log(p(x_i))=\sum_{i=1}^{n}log(\sum_{m=1}^{M}\hat{\pi_m}Bern(x_i|\mu_m))$$
$\hat{\pi_m}Bern(x_i|\mu_m)$ has already been computed. A vector named $w\_sum$ was created to store $\sum_{m=1}^{M}\hat{\pi_m}Bern(x_i|\mu_m)$
$$w\_sum[i]=\sum_{m=1}^{M}\hat{\pi_m}Bern(x_i|\mu_m)$$

### ML parameter estimation  
Just follow the equations 10.16 a and b in the slide.     

The true $\mu$ shows in the graph below.  
```{r, Assignment2_1, echo = FALSE}
set.seed(1234567890)
max_it <- 100 # max number of EM iterations
min_change <- 0.1 # min change in log lik between two consecutive iterations
n=1000 # number of training points
D=10 # number of dimensions
x <- matrix(nrow=n, ncol=D) # training data
true_pi <- vector(length = 3) # true mixing coefficients
true_mu <- matrix(nrow=3, ncol=D) # true conditional distributions
true_pi=c(1/3, 1/3, 1/3)
true_mu[1,]=c(0.5,0.6,0.4,0.7,0.3,0.8,0.2,0.9,0.1,1)
true_mu[2,]=c(0.5,0.4,0.6,0.3,0.7,0.2,0.8,0.1,0.9,0)
true_mu[3,]=c(0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5)
plot(true_mu[1,], type="o", col="blue", ylim=c(0,1), ylab = expression("value "*mu), main = expression("True " * mu))
points(true_mu[2,], type="o", col="red")
points(true_mu[3,], type="o", col="green")
#legend("bottomleft", legend = c(expression(mu[1]), expression(mu[2]), expression(mu[3])), col = c("blue", "red", "green"), pch = 1, bty = "n")
```

```{r, Assignment2_2, echo = FALSE}

# Producing the training data
for(i in 1:n) {
  m <- sample(1:3,1,prob=true_pi)
  for(d in 1:D) {
    x[i,d] <- rbinom(1,1,true_mu[m,d])
  }
}
```


## when M=3      
```{r, Assignment2_3, echo = FALSE, fig.show='hold'}

M=3 # number of clusters
w <- matrix(nrow=n, ncol=M) # weights
pi <- vector(length = M) # mixing coefficients
mu <- matrix(nrow=M, ncol=D) # conditional distributions
llik <- vector(length = max_it) # log likelihood of the EM iterations
# Random initialization of the parameters
pi <- runif(M,0.49,0.51)
pi <- pi / sum(pi)
for(m in 1:M) {
  mu[m,] <- runif(D,0.49,0.51)
}
pi
mu

# create this vector to store value
w_sum <- vector(length = n)
# make labels and margins smaller
par(cex=0.8, mar=c(2,2,0.5,1), mfrow = c(3, 3))

for(it in 1:max_it) {
  plot(mu[1,], type="o", col="blue", ylim=c(0,1), ylab = expression("value "*mu))
  points(mu[2,], type="o", col="red")
  points(mu[3,], type="o", col="green")
  #points(mu[4,], type="o", col="yellow")
  Sys.sleep(0.5)
  # E-step: Computation of the weights
  for (i in 1:n) {
    for (m in 1:M) {
      # log operation for convenience
      w[i, m] <- log(pi[m]) + sum(x[i, ] * log(mu[m, ]) + (1 - x[i, ]) * log(1 - mu[m, ]))
    }
    # exp operation for setting value back
    w[i, ] <- exp(w[i, ])
    # get the real w[i,m]
    w_sum[i] <- sum(w[i, ])
    w[i, ] <- w[i, ] / w_sum[i]
  }
  #Log likelihood computation.
  llik[it] <- sum(log(w_sum))
  
  cat("iteration: ", it, "log likelihood: ", llik[it], "\n")
  flush.console()
  # Stop if the lok likelihood has not changed significantly
  if (it > 1 && abs(llik[it] - llik[it - 1]) < min_change) {
    break
  }
  #M-step: ML parameter estimation from the data and weights
  pi <- colMeans(w)
  for (m in 1:M) {
    mu[m, ] <- colSums(w[, m] * x) / sum(w[, m])
  }
}
```


```{r, Assignment2_4, echo = FALSE}

cat("The final pi is:\n")
pi
cat("The final mu is:\n")
mu
plot(llik[1:it], type="o")
```


## when M=2      

```{r, Assignment2_5, echo = FALSE, fig.show='hold'}

M=2 # number of clusters
w <- matrix(nrow=n, ncol=M) # weights
pi <- vector(length = M) # mixing coefficients
mu <- matrix(nrow=M, ncol=D) # conditional distributions
llik <- vector(length = max_it) # log likelihood of the EM iterations
# Random initialization of the parameters
pi <- runif(M,0.49,0.51)
pi <- pi / sum(pi)
for(m in 1:M) {
  mu[m,] <- runif(D,0.49,0.51)
}
pi
mu

# create this vector to store value
w_sum <- vector(length = n)
# make labels and margins smaller
par(cex=0.8, mar=c(2,2,0.5,1), mfrow = c(3, 3))

for(it in 1:max_it) {
  plot(mu[1,], type="o", col="blue", ylim=c(0,1), ylab = expression("value "*mu))
  points(mu[2,], type="o", col="red")
 # points(mu[3,], type="o", col="green")
  #points(mu[4,], type="o", col="yellow")
  Sys.sleep(0.5)
  # E-step: Computation of the weights
  for (i in 1:n) {
    for (m in 1:M) {
      # log operation for convenience
      w[i, m] <- log(pi[m]) + sum(x[i, ] * log(mu[m, ]) + (1 - x[i, ]) * log(1 - mu[m, ]))
    }
    # exp operation for setting value back
    w[i, ] <- exp(w[i, ])
    # get the real w[i,m]
    w_sum[i] <- sum(w[i, ])
    w[i, ] <- w[i, ] / w_sum[i]
  }
  #Log likelihood computation.
  llik[it] <- sum(log(w_sum))
  
  cat("iteration: ", it, "log likelihood: ", llik[it], "\n")
  flush.console()
  # Stop if the lok likelihood has not changed significantly
  if (it > 1 && abs(llik[it] - llik[it - 1]) < min_change) {
    break
  }
  #M-step: ML parameter estimation from the data and weights
  pi <- colMeans(w)
  for (m in 1:M) {
    mu[m, ] <- colSums(w[, m] * x) / sum(w[, m])
  }
}
```



```{r, Assignment2_6, echo = FALSE}

cat("The final pi is:\n")
pi
cat("The final mu is:\n")
mu
plot(llik[1:it], type="o")
```



## when M=4      
```{r, Assignment2_7, echo = FALSE, fig.show='hold'}

M=4 # number of clusters
w <- matrix(nrow=n, ncol=M) # weights
pi <- vector(length = M) # mixing coefficients
mu <- matrix(nrow=M, ncol=D) # conditional distributions
llik <- vector(length = max_it) # log likelihood of the EM iterations
# Random initialization of the parameters
pi <- runif(M,0.49,0.51)
pi <- pi / sum(pi)
for(m in 1:M) {
  mu[m,] <- runif(D,0.49,0.51)
}
pi
mu

# create this vector to store value
w_sum <- vector(length = n)
# make labels and margins smaller
par(cex=0.8, mar=c(2,2,0.5,1), mfrow = c(3, 3))

for(it in 1:max_it) {
  plot(mu[1,], type="o", col="blue", ylim=c(0,1), ylab = expression("value "*mu))
  points(mu[2,], type="o", col="red")
  points(mu[3,], type="o", col="green")
  points(mu[4,], type="o", col="black")
  Sys.sleep(0.5)
  # E-step: Computation of the weights
  for (i in 1:n) {
    for (m in 1:M) {
      # log operation for convenience
      w[i, m] <- log(pi[m]) + sum(x[i, ] * log(mu[m, ]) + (1 - x[i, ]) * log(1 - mu[m, ]))
    }
    # exp operation for setting value back
    w[i, ] <- exp(w[i, ])
    # get the real w[i,m]
    w_sum[i] <- sum(w[i, ])
    w[i, ] <- w[i, ] / w_sum[i]
  }
  #Log likelihood computation.
  llik[it] <- sum(log(w_sum))
  
  cat("iteration: ", it, "log likelihood: ", llik[it], "\n")
  flush.console()
  # Stop if the lok likelihood has not changed significantly
  if (it > 1 && abs(llik[it] - llik[it - 1]) < min_change) {
    break
  }
  #M-step: ML parameter estimation from the data and weights
  pi <- colMeans(w)
  for (m in 1:M) {
    mu[m, ] <- colSums(w[, m] * x) / sum(w[, m])
  }
}
```


```{r, Assignment2_8, echo = FALSE}

cat("The final pi is:\n")
pi
cat("The final mu is:\n")
mu
plot(llik[1:it], type="o")
```


According to above R results, it can be seen that:

1) When $M=3$, the number of clusters in the model matches the true underlying structure of the data, thus in this scenario the estimated parameters can closely match the true mixing coefficients and conditional distributions(the final plot for $\mu$ distribution are quite similar to the plot for true $\mu$ distribution), and the model are able to capture the complexity of the data with three clusters. While, in terms of $M=2$ or $M=4$, the plots show that the estimated parameter deviate from the true distribution. When $M=2$, the model oversimplify the data, leading to a less accurate representation of the true distribution. And when $M=4$, the model try to fit into four clusters. As a result, the extra cluster might try to fit all the data, and overcomplicate the distribution. Besides, as the EM algorithm is for unsupervised learning, we don't the label each group orders for the training data, thus the order of final three clusters are not consistent with the order of true groups, but their distributions are similar.

2) According to the plots for log likelihood over iterations, when $M=3$, the log likelihood increase steadily at the beginning, then reach a plateau after several iterations. The $\mu$ distribution plots also align with such behavior. 

# Appendix  
## Code for Assignment 1  
### Q1
```{r ref.label=c('Assignment1_1'), echo=TRUE, eval=FALSE}

```

### Q2
```{r ref.label=c('Assignment1_2'), echo=TRUE, eval=FALSE}

```

### Q3
```{r ref.label=c('Assignment1_3'), echo=TRUE, eval=FALSE}

```


## Code for Assignment 2   
```{r ref.label=c('Assignment2_1, Assignment2_2'), echo=TRUE, eval=FALSE}

```
### when M=3  
```{r ref.label=c('Assignment2_3, Assignment2_4'), echo=TRUE, eval=FALSE}

```
### when M=2  
```{r ref.label=c('Assignment2_5, Assignment2_6'), echo=TRUE, eval=FALSE}

```
### when M=4  
```{r ref.label=c('Assignment2_7, Assignment2_8'), echo=TRUE, eval=FALSE}

```





